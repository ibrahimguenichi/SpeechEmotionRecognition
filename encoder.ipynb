{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb2f8053",
   "metadata": {},
   "source": [
    "# Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "185a8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4249013",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelDataset(Dataset):\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.mel_paths = self.df[\"mel_npy_path\"].tolist()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mel_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mel = np.load(self.mel_paths[idx])  # Shape: (n_mels, time)\n",
    "        mel = (mel - mel.min()) / (mel.max() - mel.min()) # Normalize to [0, 1]\n",
    "        mel = torch.tensor(mel, dtype=torch.float32).unsqueeze(0)  # (1, n_mels, time)\n",
    "        if self.transform:\n",
    "            mel = self.transform(mel)\n",
    "        return mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89b061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            ConvBlock(in_channels, out_channels),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72127b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim=512):\n",
    "        super().__init__()\n",
    "        self.initial_conv = ConvBlock(1, 32)      # -> (32, 128, 861)\n",
    "\n",
    "        self.down1 = DownBlock(32, 64)            # -> (64, 64, 430)\n",
    "        self.down2 = DownBlock(64, 128)           # -> (128, 32, 215)\n",
    "        self.down3 = DownBlock(128, 256)          # -> (256, 16, 107)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((4, 8))  # -> (256, 4, 8)\n",
    "        self.proj = nn.Linear(256 * 4 * 8, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "\n",
    "        x = self.initial_conv(x)\n",
    "        skips.append(x)\n",
    "\n",
    "        x = self.down1(x)\n",
    "        skips.append(x)\n",
    "\n",
    "        x = self.down2(x)\n",
    "        skips.append(x)\n",
    "\n",
    "        x = self.down3(x)\n",
    "        final_shape = x.shape[-2:]  # (H, W) = (16, 107)\n",
    "\n",
    "        x = self.pool(x)            # -> (B, 256, 4, 8)\n",
    "        x = x.flatten(1)            # -> (B, 8192)\n",
    "        x = self.proj(x)            # -> (B, embedding_dim)\n",
    "\n",
    "        return x, skips, final_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f93a406",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            ConvBlock(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class MelDecoder(nn.Module):\n",
    "    def __init__(self, embedding_dim=512, output_shape=(128, 861)):\n",
    "        super().__init__()\n",
    "        self.output_shape = output_shape\n",
    "        self.fc = nn.Linear(embedding_dim, 256 * 4 * 8)  # must match encoder output shape\n",
    "\n",
    "        self.up1 = UpBlock(256, 128)\n",
    "        self.up2 = UpBlock(128 + 128, 64)\n",
    "        self.up3 = UpBlock(64 + 64, 32)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            ConvBlock(32 + 32, 16),\n",
    "            nn.Conv2d(16, 1, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skips, final_shape):\n",
    "        B = x.size(0)\n",
    "        x = self.fc(x)                      # -> (B, 8192)\n",
    "        x = x.view(B, 256, 4, 8)            # match encoder output before flatten\n",
    "\n",
    "        skip1, skip2, skip3 = skips[2], skips[1], skips[0]\n",
    "\n",
    "        x = self.up1(x)                     # -> (B, 128, 8, 16)\n",
    "        x = self.up2(torch.cat([x, skip1], dim=1))  # concat with skip2\n",
    "        x = self.up3(torch.cat([x, skip2], dim=1))  # concat with skip3\n",
    "\n",
    "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)  # up to approx (128, 860)\n",
    "        x = self.final(torch.cat([x, skip3], dim=1))  # concat with initial conv output\n",
    "\n",
    "        x = F.interpolate(x, size=self.output_shape, mode='bilinear', align_corners=True)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b82d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelAutoencoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, skips, final_shape = self.encoder(x)\n",
    "        recon = self.decoder(z, skips, final_shape)\n",
    "        return recon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b73156e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_auto_encoder(model, train_loader, val_loader, epochs=200, checkpoint_dir='checkpoints/reference_encoder'):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(\n",
    "        list(model.parameters()),\n",
    "        lr=1e-3, weight_decay=1e-4  # L2 regularization term\n",
    "    )\n",
    "\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # ----- Training -----\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for mel in train_loader:\n",
    "            mel = mel.to(device)  # [B, 1, M, T]\n",
    "            recon = model(mel)\n",
    "            loss = F.l1_loss(recon, mel) + 0.1 * F.mse_loss(recon, mel)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # ----- Validation -----\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for mel in val_loader:\n",
    "                mel = mel.to(device)\n",
    "                recon = model(mel)\n",
    "                loss = F.l1_loss(recon, mel)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # ----- Save checkpoint -----\n",
    "        if epoch % 5 == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pth')\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Checkpoint saved to {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a885bcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"Data/split_train_val/train_split.csv\")\n",
    "val_df = pd.read_csv(\"Data/split_train_val/val_split.csv\")\n",
    "\n",
    "train_df[\"mel_npy_path\"] = train_df[\"mel_npy_path\"].apply(lambda x: x.replace(\"\\\\\", \"/\"))\n",
    "val_df[\"mel_npy_path\"] = val_df[\"mel_npy_path\"].apply(lambda x: x.replace(\"\\\\\", \"/\"))\n",
    "\n",
    "train_dataset = MelDataset(csv_path=\"Data/split_train_val/train_split.csv\")\n",
    "val_dataset = MelDataset(csv_path=\"Data/split_train_val/val_split.csv\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "830c834f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 256, 8, 54]' is invalid for input of size 262144",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m decoder = MelDecoder(embedding_dim=\u001b[32m512\u001b[39m, output_shape=(\u001b[32m128\u001b[39m, \u001b[32m861\u001b[39m)).to(device)\n\u001b[32m      5\u001b[39m autoencoder = MelAutoencoder(encoder=encoder, decoder=decoder).to(device)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mtrain_auto_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mautoencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mtrain_auto_encoder\u001b[39m\u001b[34m(model, train_loader, val_loader, epochs, checkpoint_dir)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mel \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m     15\u001b[39m     mel = mel.to(device)  \u001b[38;5;66;03m# [B, 1, M, T]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     recon = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     loss = F.l1_loss(recon, mel) + \u001b[32m0.1\u001b[39m * F.mse_loss(recon, mel)\n\u001b[32m     19\u001b[39m     optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mMelAutoencoder.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m      8\u001b[39m     z, skips, final_shape = \u001b[38;5;28mself\u001b[39m.encoder(x)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     recon = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskips\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m recon\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mMelDecoder.forward\u001b[39m\u001b[34m(self, x, skips, final_shape)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, skips, final_shape):\n\u001b[32m     21\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.fc(x)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     x = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Restore spatial dims\u001b[39;00m\n\u001b[32m     24\u001b[39m     skip1, skip2, skip3 = skips[\u001b[32m2\u001b[39m], skips[\u001b[32m1\u001b[39m], skips[\u001b[32m0\u001b[39m]\n\u001b[32m     26\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.up1(x)\n",
      "\u001b[31mRuntimeError\u001b[39m: shape '[-1, 256, 8, 54]' is invalid for input of size 262144"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "encoder = MelEncoder(embedding_dim=512).to(device)\n",
    "decoder = MelDecoder(embedding_dim=512, output_shape=(128, 861)).to(device)\n",
    "autoencoder = MelAutoencoder(encoder=encoder, decoder=decoder).to(device)\n",
    "\n",
    "train_auto_encoder(autoencoder, train_loader, val_loader, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b17e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_first_10(autoencoder, dataloader, device, checkpoint_path = 'checkpoints/reference_encoder/checkpoint_epoch_20.pth'):\n",
    "    autoencoder.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    autoencoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            original = batch.to(device)\n",
    "            reconstructed = autoencoder(original)\n",
    "            break  # Only take the first batch\n",
    "\n",
    "    original = original.cpu()\n",
    "    reconstructed = reconstructed.cpu()\n",
    "\n",
    "    # Plot the first 10 samples\n",
    "    for i in range(10):\n",
    "        plt.figure(figsize=(10, 3))\n",
    "\n",
    "        # Original\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(original[i][0], aspect='auto', origin='lower', cmap='magma')\n",
    "        plt.title(f'Original #{i+1}')\n",
    "        plt.colorbar()\n",
    "\n",
    "        # Reconstructed\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(reconstructed[i][0], aspect='auto', origin='lower', cmap='magma')\n",
    "        plt.title(f'Reconstructed #{i+1}')\n",
    "        plt.colorbar()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2837054",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_first_10(autoencoder, val_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83662831-bade-44de-ae6a-1a837dfd7e23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
