{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b167dc",
   "metadata": {},
   "source": [
    "# Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9337d298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from transformers import AutoTokenizer, AutoModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5fc2fa",
   "metadata": {},
   "source": [
    "# Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5b487ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionResNet50(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.4):\n",
    "        super(EmotionResNet50, self).__init__()\n",
    "        self.base_model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "        # Get input size of the original fully connected layer\n",
    "        in_features = self.base_model.fc.in_features\n",
    "\n",
    "        # Replace the final fc layer with Dropout + Linear\n",
    "        self.base_model.fc = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        # Forward until the layer before classification\n",
    "        x = self.base_model.conv1(x)\n",
    "        x = self.base_model.bn1(x)\n",
    "        x = self.base_model.relu(x)\n",
    "        x = self.base_model.maxpool(x)\n",
    "\n",
    "        x = self.base_model.layer1(x)\n",
    "        x = self.base_model.layer2(x)\n",
    "        x = self.base_model.layer3(x)\n",
    "        x = self.base_model.layer4(x)\n",
    "\n",
    "        x = self.base_model.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x  # This is your emotion embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4021195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(channels)\n",
    "        )\n",
    "        self.activation = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(x + self.block(x))\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_residual=True):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.residual = ResidualConvBlock(out_channels) if use_residual else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        return self.residual(x)\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            ConvBlock(in_channels, out_channels),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.attn = SkipAttention(out_channels)\n",
    "        self.conv = ConvBlock(out_channels * 2, out_channels)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        if x.shape != skip.shape:\n",
    "            x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=True)\n",
    "        skip = self.attn(skip)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class SkipAttention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels // 2, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels // 2, channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, skip):\n",
    "        return skip * self.attn(skip)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "033d41d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024):\n",
    "        super().__init__()\n",
    "        self.initial_conv = ConvBlock(1, 32)\n",
    "        self.down1 = DownBlock(32, 64)       # H/2, W/2\n",
    "        self.down2 = DownBlock(64, 128)      # H/4, W/4\n",
    "        self.down3 = DownBlock(128, 256)      # H/8, W/8\n",
    "        self.down4 = DownBlock(256, 512)      # H/16, W/16\n",
    "        self.final_block = nn.Sequential(\n",
    "            nn.MaxPool2d(2),                  # H/32, W/32\n",
    "            ConvBlock(512, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        x = self.initial_conv(x)   # -> [B, 32, H, W]\n",
    "        skips.append(x)\n",
    "\n",
    "        x = self.down1(x)          # -> [B, 64, H/2, W/2]\n",
    "        skips.append(x)\n",
    "\n",
    "        x = self.down2(x)          # -> [B, 128, H/4, W/4]\n",
    "        skips.append(x)\n",
    "\n",
    "        x = self.down3(x)          # -> [B, 256, H/8, W/8]\n",
    "        skips.append(x)\n",
    "\n",
    "        x = self.down4(x)          # -> [B, 512, H/16, W/16]\n",
    "        skips.append(x)\n",
    "\n",
    "        x = self.final_block(x)    # -> [B, 1024, H/32, W/32]\n",
    "        return x, skips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3eb286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-multilingual-cased\"):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state.mean(dim=1)  # Mean pooling over tokens\n",
    "        return pooled  # shape: [batch_size, hidden_dim]\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "009752e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModelDataset(Dataset):\n",
    "    def __init__(self, csv_path, ser_model, mel_encoder, text_model, tokenizer, max_text_len=200, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (str): Path to CSV containing paths and labels\n",
    "            ser_model: Speech Emotion Recognition model class\n",
    "            mel_encoder: MelEncoder model class\n",
    "            text_model: SmallTextTransformer instance\n",
    "            tokenizer: Text tokenizer (e.g., from transformers)\n",
    "            max_text_len: Maximum text sequence length\n",
    "            transform: Optional transforms for mel spectrograms\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.mel_paths = self.df[\"mel_npy_path\"].tolist()\n",
    "        self.labels = self.df[\"emotion_encoded\"].tolist()\n",
    "        self.texts = self.df[\"sentence\"].tolist()\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_text_len = max_text_len\n",
    "        \n",
    "        # Initialize models\n",
    "        self.ser_model = ser_model(num_classes=6).to(self.device)\n",
    "        self.mel_encoder = mel_encoder(embedding_dim=1024).to(self.device)\n",
    "        self.text_model = text_model().to(self.device)\n",
    "        \n",
    "        # Load checkpoints\n",
    "        ser_checkpoint = torch.load('models/SER.pth', map_location=self.device)\n",
    "        self.ser_model.load_state_dict(ser_checkpoint['model_state_dict'])\n",
    "        self.ser_model.eval()\n",
    "        \n",
    "        mel_checkpoint = torch.load('models/MelEncoder.pth', map_location=self.device)\n",
    "        self.mel_encoder.load_state_dict(mel_checkpoint)\n",
    "        self.mel_encoder.eval()\n",
    "        \n",
    "        self.transform = transform if transform else transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.Lambda(lambda x: x.repeat(3, 1, 1))\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mel_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load and process Mel-spectrogram\n",
    "        mel_spec = np.load(self.mel_paths[idx])\n",
    "        mel_spec = (mel_spec - mel_spec.min()) / (mel_spec.max() - mel_spec.min())\n",
    "        mel_tensor = self.transform(\n",
    "            Image.fromarray((mel_spec * 255).astype(np.uint8), mode='L')\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Input for the MelEncoder\n",
    "        mel = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        init_mel = mel.squeeze(0)\n",
    "        \n",
    "        # Process text\n",
    "        text = self.texts[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_text_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            # Audio embeddings\n",
    "            emotions_embedding = self.ser_model.get_embedding(mel_tensor.unsqueeze(0))\n",
    "            emotions_embedding = emotions_embedding.view(-1)\n",
    "            utterance_vector, _ = self.mel_encoder(mel.unsqueeze(0))\n",
    "            utterance_vector = utterance_vector.squeeze(0)\n",
    "            \n",
    "            # Text embedding\n",
    "            text_embedding = self.text_model(input_ids, attention_mask)\n",
    "            text_embedding = text_embedding.squeeze(0)\n",
    "        \n",
    "        return {\n",
    "            # 'audio_emotion': emotions_embedding.squeeze(0),\n",
    "            # 'mel_embedding': utterance_vector.squeeze(0),\n",
    "            # 'text_embedding': text_embedding.squeeze(0),\n",
    "            # 'label': torch.tensor(self.labels[idx].to(self.device))\n",
    "            'emotion_embedding': emotions_embedding,\n",
    "            'mel_embedding': utterance_vector,\n",
    "            'text_embedding': text_embedding,\n",
    "            'initial_mel': init_mel\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b35a8d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_14520\\291769843.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ser_checkpoint = torch.load('models/SER.pth', map_location=self.device)\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_14520\\291769843.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mel_checkpoint = torch.load('models/MelEncoder.pth', map_location=self.device)\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_14520\\291769843.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ser_checkpoint = torch.load('models/SER.pth', map_location=self.device)\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_14520\\291769843.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mel_checkpoint = torch.load('models/MelEncoder.pth', map_location=self.device)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DiffusionModelDataset(\n",
    "    csv_path='Data/split_train_val/train_split.csv',\n",
    "    ser_model=EmotionResNet50,  # Replace with your actual class\n",
    "    mel_encoder=MelEncoder,\n",
    "    text_model=TransformerEncoder,\n",
    "    tokenizer=tokenizer  # e.g., BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    ")\n",
    "\n",
    "val_dataset = DiffusionModelDataset(\n",
    "    csv_path='Data/split_train_val/val_split.csv',\n",
    "    ser_model=EmotionResNet50,\n",
    "    mel_encoder=MelEncoder,\n",
    "    text_model=TransformerEncoder,\n",
    "    tokenizer=tokenizer  # e.g., BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dafc1862",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdf6170e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion_embedding shape: torch.Size([2, 2048])\n",
      "mel_embedding shape: torch.Size([2, 1024, 4, 26])\n",
      "text_embedding shape: torch.Size([2, 768])\n",
      "initial me shape: torch.Size([2, 128, 861])\n"
     ]
    }
   ],
   "source": [
    "# Fetch one batch\n",
    "for batch in train_dataloader:\n",
    "    print(\"emotion_embedding shape:\", batch['emotion_embedding'].shape)\n",
    "    print(\"mel_embedding shape:\", batch['mel_embedding'].shape)\n",
    "    print(\"text_embedding shape:\", batch['text_embedding'].shape)\n",
    "    print(\"initial me shape:\", batch['initial_mel'].shape)\n",
    "\n",
    "    # # Optional: look at the data itself\n",
    "    # print(\"Sample label:\", batch['label'][0])\n",
    "    # print(\"Mel embedding (sample):\", batch['mel_embedding'][0][:5])  # Show first few values\n",
    "\n",
    "    break  # Only one batch for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d030e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingProjector(nn.Module):\n",
    "    def __init__(self, emotion_dim=2048, mel_style_dim=1024, mel_h=4, mel_w=26, text_dim=768, cond_dim=1024):\n",
    "        super(EmbeddingProjector, self).__init__()\n",
    "        self.cond_dim = cond_dim\n",
    "\n",
    "        # Projection layers for each modality\n",
    "        self.proj_emotion = nn.Linear(emotion_dim, cond_dim)\n",
    "        self.proj_text = nn.Linear(text_dim, cond_dim)\n",
    "        \n",
    "        # First projection to a larger intermediate size (e.g., 8192)\n",
    "        self.proj_mel_first = nn.Linear(mel_style_dim * mel_h * mel_w, 8192)\n",
    "        # Second projection to a middle-size (e.g., 4096)\n",
    "        self.proj_mel_intermediate = nn.Linear(8192, 4096)\n",
    "        # Final projection to cond_dim\n",
    "        self.proj_mel = nn.Linear(4096, cond_dim)\n",
    "\n",
    "    def forward(self, emotion_embedding, mel_embedding, text_embedding):\n",
    "        # emotion_embedding: [B, 2048] --> [B, cond_dim]\n",
    "        cond_emotion = self.proj_emotion(emotion_embedding)\n",
    "\n",
    "        # text_embedding: [B, 768] --> [B, cond_dim]\n",
    "        cond_text = self.proj_text(text_embedding)\n",
    "\n",
    "        # mel_embedding: [B, 1024, 4, 26] --> [B, cond_dim]\n",
    "        mel_flat = mel_embedding.view(mel_embedding.size(0), -1)  # Flatten to [B, 1024*4*26]\n",
    "        \n",
    "        # Pass through the first projection layer to a large intermediate dimension (8192)\n",
    "        mel_intermediate_first = self.proj_mel_first(mel_flat)  # [B, 8192]\n",
    "        \n",
    "        # Pass through the second projection to a medium intermediate dimension (4096)\n",
    "        mel_intermediate = self.proj_mel_intermediate(mel_intermediate_first)  # [B, 4096]\n",
    "        \n",
    "        # Final projection to cond_dim (e.g., 1024)\n",
    "        cond_mel = self.proj_mel(mel_intermediate)  # [B, cond_dim]\n",
    "\n",
    "        return cond_emotion, cond_text, cond_mel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80a38d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiLM(nn.Module):\n",
    "    def __init__(self, cond_dim, feature_dim):\n",
    "        super().__init__()\n",
    "        self.film = nn.Linear(cond_dim, feature_dim * 2)\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        gamma_beta = self.film(cond)  # [B, 2 * C]\n",
    "        gamma, beta = gamma_beta.chunk(2, dim=1)  # [B, C] each\n",
    "        gamma = gamma.unsqueeze(-1).unsqueeze(-1)\n",
    "        beta = beta.unsqueeze(-1).unsqueeze(-1)\n",
    "        return gamma * x + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69dbc8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlockWithFiLM(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, cond_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.film = FiLM(cond_dim, out_channels)\n",
    "        self.skip = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        identity = self.skip(x)\n",
    "        out = self.relu(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        out = self.film(out, cond)\n",
    "        out = self.relu(out + identity)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ba89ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diffusion(self, mel_spec, timesteps):\n",
    "    # Add noise to the mel spectrogram at each timestep\n",
    "    noise_schedule = torch.linspace(0, 1, timesteps).to(mel_spec.device)\n",
    "    noisy_mel_specs = []\n",
    "\n",
    "    for t in noise_schedule:\n",
    "        noise = torch.randn_like(mel_spec) * t\n",
    "        noisy_mel_spec = mel_spec + noise\n",
    "        noisy_mel_specs.append(noisy_mel_spec)\n",
    "\n",
    "    return torch.stack(noisy_mel_specs)  # shape: [timesteps, B, C, H, W]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62ba538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_diffusion(self, noisy_latent_tensor, emotion_embedding, mel_embedding, text_embedding, timesteps):\n",
    "    # Initialize the noisy latent tensor and embeddings\n",
    "    noise_schedule = torch.linspace(1, 0, timesteps).to(noisy_latent_tensor.device)\n",
    "    denoised_output = noisy_latent_tensor  # Start with the noisy latent tensor\n",
    "\n",
    "    for t in range(timesteps):\n",
    "        t_idx = timesteps - t - 1  # Reverse order for denoising\n",
    "\n",
    "        # Project embeddings to a common dimension\n",
    "        cond_emotion, cond_text, cond_mel = self.embedding_projector(\n",
    "            emotion_embedding, mel_embedding, text_embedding\n",
    "        )\n",
    "\n",
    "        # Apply ResNet blocks with FiLM conditioning layers (use the conditioning embeddings)\n",
    "        denoised_output = self.resnet_block_1(denoised_output, cond_emotion)\n",
    "        denoised_output = self.downsample_1(denoised_output)\n",
    "        denoised_output = self.resnet_block_2(denoised_output, cond_mel)\n",
    "        denoised_output = self.downsample_2(denoised_output)\n",
    "\n",
    "        # Denoising through the rest of the layers\n",
    "        denoised_output = self.resnet_block_3(denoised_output, cond_text)\n",
    "        denoised_output = self.upsample_1(denoised_output)\n",
    "        denoised_output = self.resnet_block_4(denoised_output, cond_mel)\n",
    "        denoised_output = self.upsample_2(denoised_output)\n",
    "        denoised_output = self.resnet_block_5(denoised_output, cond_emotion)\n",
    "\n",
    "        # Final convolution layer to generate the output\n",
    "        denoised_output = self.final_conv(denoised_output)\n",
    "\n",
    "        # Gradually reduce the noise according to the noise schedule\n",
    "        denoised_output = denoised_output / (1 - noise_schedule[t_idx])\n",
    "\n",
    "    return denoised_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0906cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDiffusionModel(nn.Module):\n",
    "    def __init__(self, emotion_dim=2048, mel_style_dim=1024, mel_h=4, mel_w=26, text_dim=768, cond_dim=1024):\n",
    "        super(EmotionDiffusionModel, self).__init__()\n",
    "\n",
    "        # Embedding Projector (projects input embeddings to a common dimension)\n",
    "        self.embedding_projector = EmbeddingProjector(\n",
    "            emotion_dim=emotion_dim,\n",
    "            mel_style_dim=mel_style_dim,\n",
    "            mel_h=mel_h,\n",
    "            mel_w=mel_w,\n",
    "            text_dim=text_dim,\n",
    "            cond_dim=cond_dim\n",
    "        )\n",
    "\n",
    "        # Initial convolution layer (typically applied to the noisy latent tensor)\n",
    "        self.initial_conv = nn.Conv2d(1, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        # Downsample/upsample blocks\n",
    "        self.downsample_1 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
    "        self.downsample_2 = nn.Conv2d(512, 1024, kernel_size=3, stride=2, padding=1)\n",
    "        self.upsample_1 = nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.upsample_2 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        # Final convolution layer to match output dimension\n",
    "        self.final_conv = nn.Conv2d(256, 1, kernel_size=3, padding=1)\n",
    "\n",
    "        # ResNet blocks with FiLM conditioning layers\n",
    "        self.resnet_block_1 = ResNetBlockWithFiLM(256, 256, cond_dim)\n",
    "        self.resnet_block_2 = ResNetBlockWithFiLM(512, 512, cond_dim)\n",
    "        self.resnet_block_3 = ResNetBlockWithFiLM(1024, 1024, cond_dim)\n",
    "        self.resnet_block_4 = ResNetBlockWithFiLM(512, 512, cond_dim)\n",
    "        self.resnet_block_5 = ResNetBlockWithFiLM(256, 256, cond_dim)\n",
    "\n",
    "    def forward(self, emotion_embedding, mel_embedding, text_embedding, noisy_latent_tensor, timesteps):\n",
    "        # Reverse Diffusion\n",
    "        return self.reverse_diffusion(noisy_latent_tensor, emotion_embedding, mel_embedding, text_embedding, timesteps)\n",
    "\n",
    "    def forward_diffusion(self, mel_spec, timesteps):\n",
    "        return self.forward_diffusion(mel_spec, timesteps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ccbd9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with desired dimensions\n",
    "emotion_dim = 2048  # Emotion embedding dimension\n",
    "mel_style_dim = 1024  # Mel-style embedding dimension\n",
    "mel_h, mel_w = 4, 26  # Mel spectrogram dimensions\n",
    "text_dim = 768  # Text embedding dimension\n",
    "cond_dim = 1024  # Common embedding dimension for conditioning\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = EmotionDiffusionModel(\n",
    "    emotion_dim=emotion_dim,\n",
    "    mel_style_dim=mel_style_dim,\n",
    "    mel_h=mel_h,\n",
    "    mel_w=mel_w,\n",
    "    text_dim=text_dim,\n",
    "    cond_dim=cond_dim\n",
    ").to(device)  # move to GPU if available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3d7c170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs, device, save_dir, save_every=5):\n",
    "    \"\"\"\n",
    "    Trains the diffusion model with emotional conditioning using composite loss.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The EmotionDiffusionModel.\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        val_loader (DataLoader): DataLoader for validation data.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        device (torch.device): Training device (CPU or GPU).\n",
    "        save_dir (str): Directory to save checkpoints.\n",
    "        save_every (int): Save model every `save_every` epochs.\n",
    "\n",
    "    Returns:\n",
    "        model: Trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=1e-3, weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "    # Define both loss functions\n",
    "    mse_loss = nn.MSELoss()\n",
    "    l1_loss = nn.L1Loss()\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # -------------------- Train --------------------\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            emotion_embedding = batch['emotion_embedding'].to(device)\n",
    "            mel_embedding = batch['mel_embedding'].to(device)\n",
    "            text_embedding = batch['text_embedding'].to(device)\n",
    "            mel_spec = batch['initial_mel'].to(device)  # Ground truth mel-spectrogram\n",
    "\n",
    "            # Forward diffusion: add noise\n",
    "            noisy_mel_specs = model.forward_diffusion(mel_spec, timesteps=100)\n",
    "            noisy_latent_tensor = noisy_mel_specs[-1]\n",
    "\n",
    "            # Reverse diffusion: denoise\n",
    "            output = model(emotion_embedding, mel_embedding, text_embedding,\n",
    "                           noisy_latent_tensor, timesteps=100)\n",
    "\n",
    "            # Composite loss\n",
    "            loss = 0.7 * mse_loss(output, mel_spec) + 0.3 * l1_loss(output, mel_spec)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # -------------------- Evaluation --------------------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                emotion_embedding = batch['emotion_embedding'].to(device)\n",
    "                mel_embedding = batch['mel_embedding'].to(device)\n",
    "                text_embedding = batch['text_embedding'].to(device)\n",
    "                mel_spec = batch['mel_spec'].to(device)\n",
    "\n",
    "                noisy_mel_specs = model.forward_diffusion(mel_spec, timesteps=100)\n",
    "                noisy_latent_tensor = noisy_mel_specs[-1]\n",
    "\n",
    "                output = model(emotion_embedding, mel_embedding, text_embedding,\n",
    "                               noisy_latent_tensor, timesteps=100)\n",
    "\n",
    "                # Composite loss for validation\n",
    "                loss = 0.7 * mse_loss(output, mel_spec) + 0.3 * l1_loss(output, mel_spec)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        # -------------------- Logging --------------------\n",
    "        print(f\"Epoch {epoch:3d} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # -------------------- Checkpoint --------------------\n",
    "        if epoch % save_every == 0:\n",
    "            checkpoint_path = os.path.join(save_dir, f\"model_epoch_{epoch}.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved to: {checkpoint_path}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0916ebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diffusion(self, mel_spec, timesteps):\n",
    "    return self.add_noise(mel_spec, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2842af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00f79478",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trained_model = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_dataloader,\n",
    "    val_loader=val_dataloader,\n",
    "    num_epochs=50,\n",
    "    device=device,\n",
    "    save_dir=\"./checkpoints/diffusion_model\",\n",
    "    \n",
    "    save_every=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba0c1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_checkpoint(model, checkpoint_dir, device):\n",
    "    best_loss = float('inf')\n",
    "    best_path = None\n",
    "\n",
    "    # Loop over all checkpoint files\n",
    "    for filename in os.listdir(checkpoint_dir):\n",
    "        if filename.endswith(\".pt\"):\n",
    "            path = os.path.join(checkpoint_dir, filename)\n",
    "            checkpoint = torch.load(path, map_location=device)\n",
    "            val_loss = checkpoint.get(\"val_loss\", float(\"inf\"))\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_path = path\n",
    "\n",
    "    if best_path is None:\n",
    "        raise FileNotFoundError(\"No valid checkpoints found.\")\n",
    "\n",
    "    # Load the best model\n",
    "    print(f\"Loading best checkpoint: {best_path} (val_loss = {best_loss:.4f})\")\n",
    "    checkpoint = torch.load(best_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_on_first_5(model, val_loader, device):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            for i in range(batch['mel_spec'].size(0)):\n",
    "                if count >= 5:\n",
    "                    return  # Only process 5 samples\n",
    "\n",
    "                emotion_embedding = batch['emotion_embedding'][i:i+1].to(device)\n",
    "                mel_embedding = batch['mel_embedding'][i:i+1].to(device)\n",
    "                text_embedding = batch['text_embedding'][i:i+1].to(device)\n",
    "                mel_spec = batch['mel_spec'][i:i+1].to(device)\n",
    "\n",
    "                noisy_mel = model.forward_diffusion(mel_spec, timesteps=100)[-1]\n",
    "                predicted = model(emotion_embedding, mel_embedding, text_embedding,\n",
    "                                  noisy_mel, timesteps=100)\n",
    "\n",
    "                print(f\"\\nSample #{count+1}\")\n",
    "                print(f\"Predicted shape: {predicted.shape}\")\n",
    "                print(f\"Target shape:    {mel_spec.shape}\")\n",
    "\n",
    "                # You can optionally convert to numpy or save audio/mel images here\n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0208c728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have model architecture defined as EmotionDiffusionModel\n",
    "model = EmotionDiffusionModel(...)  # fill in required args\n",
    "\n",
    "# Load best checkpoint\n",
    "model = load_best_checkpoint(model, checkpoint_dir=\"./checkpoints/diffusion_model\", device=device)\n",
    "\n",
    "# Test on first 5 validation samples\n",
    "test_model_on_first_5(model, val_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c2aac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_on_first_5(model, val_loader, device, save_dir=\"results\"):\n",
    "    model.eval()\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            for i in range(batch['mel_spec'].size(0)):\n",
    "                if count >= 5:\n",
    "                    return  # Only process 5 samples\n",
    "\n",
    "                # Extract the i-th sample from the batch\n",
    "                emotion_embedding = batch['emotion_embedding'][i:i+1].to(device)\n",
    "                mel_embedding = batch['mel_embedding'][i:i+1].to(device)\n",
    "                text_embedding = batch['text_embedding'][i:i+1].to(device)\n",
    "                mel_spec = batch['mel_spec'][i:i+1].to(device)\n",
    "\n",
    "                # Forward and reverse diffusion\n",
    "                noisy_mel = model.forward_diffusion(mel_spec, timesteps=100)[-1]\n",
    "                predicted = model(emotion_embedding, mel_embedding, text_embedding,\n",
    "                                  noisy_mel, timesteps=100)\n",
    "\n",
    "                # Convert predicted tensor to numpy and save\n",
    "                predicted_np = predicted.squeeze(0).cpu().numpy()  # shape: [1, 80, T] -> [80, T]\n",
    "                save_path = os.path.join(save_dir, f\"predicted_mel_{count+1}.npy\")\n",
    "                np.save(save_path, predicted_np)\n",
    "\n",
    "                print(f\"Sample #{count+1} saved to {save_path}\")\n",
    "                count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
